{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3759d02-de65-4632-8a32-5261530d9c35",
   "metadata": {},
   "source": [
    "# E2E recsys with matching engine and TFRS\n",
    "\n",
    "\n",
    "Simple example, goal being:\n",
    "\n",
    "    1) Train a Two-Tower model using movielens data\n",
    "    \n",
    "    2) Deploy the query model endpoint\n",
    "    \n",
    "    3) Save movie embeddings to json, for use in matching engine\n",
    "    \n",
    "    \n",
    "#### Note on VPC Pairing - insturctions for in-notebook pairing [here](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/matching_engine/sdk_matching_engine_for_indexing.ipynb)\n",
    "    \n",
    "First we will create a user-managed notebook behind the already created peered VPC network used for Matching Engine. Select tensorflow enterprise 2.6 with a T4 GPU\n",
    "\n",
    "\n",
    "![](./create-workbench.png)\n",
    "\n",
    "\n",
    "##### Be sure to create the notebook in the peered network\n",
    "\n",
    "\n",
    "![](./network-create.png)\n",
    "\n",
    "    \n",
    "The next notebook will connect matching engine with the query endpoint for a simple recommender system\n",
    "\n",
    "Run the below pip install one time to install tensorflow-recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1643d1c-9361-4880-8d43-d363f68da0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-recommenders==0.6.0\n",
      "  Downloading tensorflow_recommenders-0.6.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.6.0) (2.6.5)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.6.0) (0.15.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.1.6->tensorflow-recommenders==0.6.0) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.3.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions<3.11,>=3.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.10.0.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.1.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.19.5)\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.6.0)\n",
      "Requirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (5.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.48.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.1.2)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.4.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.12.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.19.4)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.6.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.2.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (59.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (4.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (4.11.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.2.0)\n",
      "Installing collected packages: tensorflow-recommenders\n",
      "Successfully installed tensorflow-recommenders-0.6.0\n"
     ]
    }
   ],
   "source": [
    "# !echo Y | pip uninstall tensorflow\n",
    "!pip install tensorflow-recommenders==0.6.0 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd3e5a-545d-4d7b-87de-cb6422d08285",
   "metadata": {},
   "source": [
    "### Important - restart the kernel after installing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54877358-e37c-4404-9ab2-ca7360f9ef49",
   "metadata": {},
   "source": [
    "# Train a 2 tower model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d39132d-f08e-4d2c-be54-75e5067e8b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 4.70 MiB (download: 4.70 MiB, generated: 32.41 MiB, total: 37.10 MiB) to /home/jupyter/tensorflow_datasets/movielens/100k-ratings/0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318035d0e12248afaf98f940f68d8110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90b09375a0041c89e4751a1a07abd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9c97fae74148b784f1d8024b896a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling movielens-train.tfrecord...:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to /home/jupyter/tensorflow_datasets/movielens/100k-ratings/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-08 20:18:29.188381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-08 20:18:29.284572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-08 20:18:29.286464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-08 20:18:29.291356: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-08 20:18:29.291996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-08 20:18:29.293757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-08 20:18:29.295431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-08 20:18:31.316007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-08 20:18:31.318069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-08 20:18:31.319818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-08 20:18:31.321524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13642 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 4.70 MiB (download: 4.70 MiB, generated: 150.35 KiB, total: 4.84 MiB) to /home/jupyter/tensorflow_datasets/movielens/100k-movies/0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af6b8c8bb074fe4bbc1c531d98c62e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c471093ae48b494a833efe07c17f1895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e14eeb9b6f4024a4896a5ec7a58ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/1682 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling movielens-train.tfrecord...:   0%|          | 0/1682 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to /home/jupyter/tensorflow_datasets/movielens/100k-movies/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-08 20:18:34.461376: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 27s 193ms/step - factorized_top_k/top_1_categorical_accuracy: 3.7500e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0012 - factorized_top_k/top_10_categorical_accuracy: 0.0044 - factorized_top_k/top_50_categorical_accuracy: 0.0465 - factorized_top_k/top_100_categorical_accuracy: 0.1083 - loss: 7100.8834 - regularization_loss: 0.0000e+00 - total_loss: 7100.8834\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 20s 197ms/step - factorized_top_k/top_1_categorical_accuracy: 6.2500e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0055 - factorized_top_k/top_10_categorical_accuracy: 0.0154 - factorized_top_k/top_50_categorical_accuracy: 0.1107 - factorized_top_k/top_100_categorical_accuracy: 0.2197 - loss: 6583.7061 - regularization_loss: 0.0000e+00 - total_loss: 6583.7061\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 16s 158ms/step - factorized_top_k/top_1_categorical_accuracy: 5.1250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0099 - factorized_top_k/top_10_categorical_accuracy: 0.0254 - factorized_top_k/top_50_categorical_accuracy: 0.1576 - factorized_top_k/top_100_categorical_accuracy: 0.2853 - loss: 6413.4934 - regularization_loss: 0.0000e+00 - total_loss: 6413.4934\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 14s 146ms/step - factorized_top_k/top_1_categorical_accuracy: 5.0000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0118 - factorized_top_k/top_10_categorical_accuracy: 0.0326 - factorized_top_k/top_50_categorical_accuracy: 0.1964 - factorized_top_k/top_100_categorical_accuracy: 0.3407 - loss: 6306.1999 - regularization_loss: 0.0000e+00 - total_loss: 6306.1999\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 14s 146ms/step - factorized_top_k/top_1_categorical_accuracy: 6.3750e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0129 - factorized_top_k/top_10_categorical_accuracy: 0.0362 - factorized_top_k/top_50_categorical_accuracy: 0.2215 - factorized_top_k/top_100_categorical_accuracy: 0.3774 - loss: 6226.3725 - regularization_loss: 0.0000e+00 - total_loss: 6226.3725\n",
      "20/20 [==============================] - 5s 142ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 3.0000e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0017 - factorized_top_k/top_50_categorical_accuracy: 0.0558 - factorized_top_k/top_100_categorical_accuracy: 0.1546 - loss: 6877.3619 - regularization_loss: 0.0000e+00 - total_loss: 6877.3619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.0003000000142492354,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0017000000225380063,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.05575000122189522,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.1545500010251999,\n",
       " 'loss': 3468.625,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 3468.625}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "\n",
    "from google.cloud import aiplatform_v1beta1 #needed for matching engine calls\n",
    "from google.protobuf import struct_pb2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "DIMENSIONS = 64 # this is how large the embedding dimensions get\n",
    "\n",
    "\n",
    "# Ratings data.\n",
    "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
    "\n",
    "# Select the basic features.\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_id\": tf.strings.to_number(x[\"movie_id\"]),\n",
    "    \"user_id\": tf.strings.to_number(x[\"user_id\"])\n",
    "})\n",
    "movies = movies.map(lambda x: tf.strings.to_number(x[\"movie_id\"]))\n",
    "\n",
    "# Build a model.\n",
    "class Model(tfrs.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up user representation.\n",
    "        self.user_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(\n",
    "            input_dim=2000, output_dim=DIMENSIONS),\n",
    "            ])\n",
    "        # Set up movie representation.\n",
    "        self.item_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(\n",
    "            input_dim=2000, output_dim=DIMENSIONS),\n",
    "        ])\n",
    "        # Set up a retrieval task and evaluation metrics over the\n",
    "        # entire dataset of candidates.\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=movies.batch(128).map(self.item_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        movie_embeddings = self.item_model(features[\"movie_id\"])\n",
    "\n",
    "        return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# Randomly shuffle data and split between train and test.\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "# Train.\n",
    "model.fit(train.batch(1024), epochs=5)\n",
    "\n",
    "# Evaluate.\n",
    "model.evaluate(test.batch(1024), return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbaec34-8a04-404d-aaa7-e8fe0eb82797",
   "metadata": {},
   "source": [
    "### Set your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12134d47-284d-4e55-8476-df3db335de48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://end-to-end-two-tower-wayfair/...\n"
     ]
    }
   ],
   "source": [
    "#create a bucket one-time\n",
    "# ! gsutil mb -l us-central1 gs://end-to-end-two-tower-wayfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "367695ef-4330-4dee-a296-94ac68850744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/wayfair-361917/locations/us-central1'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT = 'wayfair-361917' #set to your own\n",
    "NETWORK_NAME = 'matching-engine-vpc' #same as VPC peered network\n",
    "\n",
    "### Create a bucket to store our embeddings and models\n",
    "BUCKET = 'gs://end-to-end-two-tower-wayfair' # TODO - change for each user\n",
    "EMBEDDINGS = os.path.join(BUCKET, 'embeddings')\n",
    "QUERY_MODEL = os.path.join(BUCKET, 'query_model')\n",
    "REGION = 'us-central1'\n",
    "\n",
    "## Gets an auth token with the Parent variable\n",
    "PROJECT_ID = PROJECT\n",
    "AUTH_TOKEN = !gcloud auth print-access-token\n",
    "PROJECT_NUMBER = ! gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
    "\n",
    "\n",
    "PARENT = \"projects/{}/locations/{}\".format(PROJECT_ID, REGION)\n",
    "PARENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c97eedb-0d0d-4eea-a90f-ed3fd162b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run one time to create your bucket\n",
    "# !gsutil mb -l $REGION $BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03c40d03-6ee1-4e50-8742-1455bcfaf632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the query/user model\n",
    "\n",
    "model.user_model.save(QUERY_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "11ac9bbe-d100-4db9-aa6a-97889b540ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://end-to-end-two-tower-wayfair/query_model/\n",
      "gs://end-to-end-two-tower-wayfair/query_model/keras_metadata.pb\n",
      "gs://end-to-end-two-tower-wayfair/query_model/saved_model.pb\n",
      "gs://end-to-end-two-tower-wayfair/query_model/assets/\n",
      "gs://end-to-end-two-tower-wayfair/query_model/variables/\n"
     ]
    }
   ],
   "source": [
    "# Make sure it saved\n",
    "!gsutil ls $QUERY_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "14604762-46fd-413b-97fe-dd3c3e4ce182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "model_gcp = aiplatform.Model.upload(\n",
    "        display_name=\"Movielens User Query Model\",\n",
    "        artifact_uri=QUERY_MODEL,\n",
    "        serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest',\n",
    "        description=\"Top of the query tower, meant to return an embedding for each user instance\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10f87b1b-5686-4640-bac9-aa2b4d836417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Model object at 0x7fb918ae2550> \n",
       "resource name: projects/169420424915/locations/us-central1/models/5086793788882419712"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate the model type output\n",
    "model_gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "df3f621f-60a6-47d3-b7a3-49082f6c9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24a05ae9-6deb-4db3-91f2-41676067f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=\"Movielens Model Endpoint\",\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "06ddcf4a-c5a2-4593-9895-93b811081a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = model_gcp.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=\"Movielens User Query Model\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=2,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=0,\n",
    "    sync=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4ed84db-5c12-44fe-ad93-580b9bf7a736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7fb912e84cd0> \n",
       "resource name: projects/169420424915/locations/us-central1/endpoints/1615516832737787904"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb85c3-18b9-470d-9880-bd91b3d22564",
   "metadata": {},
   "source": [
    "## Save the embeddings for the movie dataset\n",
    "\n",
    "### Write embeddings to local storage\n",
    "Following this format for Matching Engine\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/matching_engine/sdk_matching_engine_for_indexing.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "97c49bab-4aa0-484c-84cf-ceebe88c5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_embs = movies.batch(1000).map(lambda x: [x, model.item_model(x)]).unbatch() #process 1000 at a time then flatten it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5eed5978-7ee0-48e1-a98a-08119a52989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to local disk\n",
    "with open(\"movie_embeddings.json\", 'w') as f:\n",
    "    for movie_id, movie_emb in movie_embs:\n",
    "        # print(movie_id.numpy(), movie_emb.numpy())\n",
    "        f.write('{\"id\":\"' + str(movie_id.numpy()) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(movie_emb.numpy())) + ']}')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccbe63-9f63-4da0-8314-30244c7a8df1",
   "metadata": {},
   "source": [
    "You should now see .json data as required by matching engine\n",
    "![](jsonl.png)\n",
    "\n",
    "### Upload the data to GCS\n",
    "Only remove if you have issues uploading the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd0c7011-dbf2-4f73-adcc-89aff7ecb7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://movie_embeddings.json [Content-Type=application/json]...\n",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \n",
      "Operation completed over 1 objects/1.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp movie_embeddings.json $EMBEDDINGS/movie_embeddings.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1af7d-d6db-4bd6-91d6-f8343700ea94",
   "metadata": {},
   "source": [
    "# Next we will deploy our movie inidicies. With Matching Engine\n",
    "* Create an index (from the `json` files)\n",
    "* Create and endpoint\n",
    "* Deploy the index to the endpoint so you can perform vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c678c72-3604-4d32-b1f9-7639066932e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_endpoint_me = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "index_client = aiplatform_v1beta1.IndexServiceClient(\n",
    "    client_options=dict(api_endpoint=api_endpoint_me)\n",
    ")\n",
    "\n",
    "\n",
    "DISPLAY_NAME = f\"Movielens Movie: {DIMENSIONS} DIMENSIONS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49483180-e16a-48ac-9954-557dc2718144",
   "metadata": {},
   "source": [
    "Set the Nearest Neighbor Options\n",
    "\n",
    "See here for tips on [tuning the index](https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#tuning_the_index)\n",
    "\n",
    "Other best practices from our PM team:\n",
    "```\n",
    "Start from leafNodesToSearchPercent=5 and approximateNeighborsCount=10 * k\n",
    "\n",
    "use default values for others.\n",
    "\n",
    "measure performance and recall and change those 2 parameters accordingly.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76ed453f-d1de-4a53-af86-ebd74785d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "treeAhConfig = struct_pb2.Struct(\n",
    "    fields={\n",
    "        \"leafNodeEmbeddingCount\": struct_pb2.Value(number_value=20),\n",
    "        \"leafNodesToSearchPercent\": struct_pb2.Value(number_value=7),\n",
    "    }\n",
    ")\n",
    "\n",
    "algorithmConfig = struct_pb2.Struct(\n",
    "    fields={\"treeAhConfig\": struct_pb2.Value(struct_value=treeAhConfig)}\n",
    ")\n",
    "\n",
    "config = struct_pb2.Struct(\n",
    "    fields={\n",
    "        \"dimensions\": struct_pb2.Value(number_value=DIMENSIONS),\n",
    "        \"approximateNeighborsCount\": struct_pb2.Value(number_value=10),\n",
    "        \"distanceMeasureType\": struct_pb2.Value(string_value=\"DOT_PRODUCT_DISTANCE\"),\n",
    "        \"algorithmConfig\": struct_pb2.Value(struct_value=algorithmConfig),\n",
    "    }\n",
    ")\n",
    "\n",
    "metadata = struct_pb2.Struct(\n",
    "    fields={\n",
    "        \"config\": struct_pb2.Value(struct_value=config),\n",
    "        \"contentsDeltaUri\": struct_pb2.Value(string_value=EMBEDDINGS),\n",
    "    }\n",
    ")\n",
    "\n",
    "ann_index = {\n",
    "    \"display_name\": DISPLAY_NAME,\n",
    "    \"description\": f\"Movielens {DIMENSIONS}\",\n",
    "    \"metadata\": struct_pb2.Value(struct_value=metadata),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a429a9e9-85b4-420d-a6ac-200e45cb61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_index = index_client.create_index(parent=PARENT, index=ann_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c66dcd9-32e7-468b-97f2-8acf9ed3c1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n"
     ]
    }
   ],
   "source": [
    "# Poll the operation until it's done successfullly.\n",
    "# This will take ~40 min.\n",
    "import time \n",
    "\n",
    "while True:\n",
    "    if ann_index.done():\n",
    "        break\n",
    "    print(\"Poll the operation to create index...\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea507c13-1434-4e7c-88ae-d8fd0e6e7665",
   "metadata": {},
   "source": [
    "## Note on the advantages of the algorithm\n",
    "\n",
    "[link](https://arxiv.org/pdf/1908.10396.pdf)\n",
    "\n",
    "```However, it is easy to see that not all pairs of (x, q) are equally important. The approximation error on the pairs which have a high inner product is far more important since they are likely to be among the top ranked pairs and can greatly affect the search result, while for the pairs whose inner product is low the approximation error matters much less. In other words, for a given datapoint x, we should quantize it with a bigger focus on its error with those queries which have high inner product with x. See Figure 1 for the illustration.```\n",
    "\n",
    "\n",
    "![](./algo.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5016aed4-0ca8-4a08-86f4-aa86f5ca2a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.api_core.operation.Operation at 0x7fb9180882d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "355d7d95-6067-43fc-afc9-b9a75510d6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/169420424915/locations/us-central1/indexes/1102168047868706816\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_index.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af30e4-6572-4843-8539-f9801bf89609",
   "metadata": {},
   "source": [
    "### Save the name of the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34c7bcbe-6bc5-4740-ae7d-a7bc40389ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/169420424915/locations/us-central1/indexes/1102168047868706816'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_RESOURCE_NAME = ann_index.result().name\n",
    "INDEX_RESOURCE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bab2c7-c7ca-45d6-ae49-00903d13d0a7",
   "metadata": {},
   "source": [
    "Debugging tool in case you run into issues. Example usage below.\n",
    "`!gcloud beta ai operations describe 4122851463774863360 --index=7253099976438317056 --project=$PROJECT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae2c000-792b-4766-9562-fddf93387308",
   "metadata": {},
   "source": [
    "## Create Index Endpoint and Deploy Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31a02349-03dd-4e77-8935-59aa7ca7da5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WARNING: The following filter keys were not present in any resource : fPROJECT_ID'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "596eea7a-c339-45ee-9443-e4777e073bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/169420424915/global/networks/matching-engine-vpc'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VPC_NETWORK_NAME = \"projects/{}/global/networks/{}\".format(PROJECT_NUMBER, NETWORK_NAME)\n",
    "VPC_NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "95f6267d-e920-475a-899e-25811f4eefbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint = {\n",
    "    \"display_name\": \"index_endpoint_for_demo\",\n",
    "    \"network\": VPC_NETWORK_NAME,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3959ed5b-8122-4840-8efc-6efed67f74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint_client = aiplatform_v1beta1.IndexEndpointServiceClient(\n",
    "    client_options=dict(api_endpoint=api_endpoint_me)\n",
    ")\n",
    "\n",
    "ann_index_en = index_endpoint_client.create_index_endpoint(\n",
    "    parent=PARENT, index_endpoint=index_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d70e1ef-2abd-43b1-827b-b884f34624df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/169420424915/locations/us-central1/indexEndpoints/4842407538399903744\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_index_en.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b36a5661-9fd5-4b9a-9922-6aae5b68af00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/169420424915/locations/us-central1/indexEndpoints/4842407538399903744'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_ENDPOINT_NAME = ann_index_en.result().name\n",
    "INDEX_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "beaf193d-bd0e-40be-80fa-dd4dbe641cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_INDEX_ID = 'movielens_deployed2'\n",
    "\n",
    "deploy_ann_index = {\n",
    "    \"id\": DEPLOYED_INDEX_ID,\n",
    "    \"display_name\": DEPLOYED_INDEX_ID,\n",
    "    \"index\": INDEX_RESOURCE_NAME,\n",
    "}\n",
    "r = index_endpoint_client.deploy_index(\n",
    "    index_endpoint=INDEX_ENDPOINT_NAME, deployed_index=deploy_ann_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b31a2-d4d9-40c5-9d49-224b120c4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc5328-aee5-4042-ba33-95604e74e196",
   "metadata": {},
   "source": [
    "# Connect Matching Engine and The User Model Into a Recommendation System\n",
    "\n",
    "This will bring it all together by incorporating the prediction endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d3663265-9097-4ab5-8ed5-b6065ceecec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish index_endpoint -IMPORTANT for constructing already created endpoints/indicies/etc...\n",
    "ME_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(INDEX_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "78b86481-e4fa-4c9c-b37d-8ab691f08574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.184652567,\n",
       "  -0.397861391,\n",
       "  -0.129475906,\n",
       "  -0.275661528,\n",
       "  0.0817652121,\n",
       "  0.128327087,\n",
       "  -0.264759898,\n",
       "  0.217002064,\n",
       "  -0.374049485,\n",
       "  -0.410740137,\n",
       "  -1.07684159,\n",
       "  0.110065386,\n",
       "  0.350892216,\n",
       "  -0.476837337,\n",
       "  0.227980882,\n",
       "  0.220079392,\n",
       "  -0.272344977,\n",
       "  -0.109302461,\n",
       "  0.19338505,\n",
       "  -0.696584523,\n",
       "  0.603061438,\n",
       "  -0.543672442,\n",
       "  0.192285746,\n",
       "  0.155405462,\n",
       "  0.4221223,\n",
       "  0.289614797,\n",
       "  -0.58251214,\n",
       "  0.107899651,\n",
       "  -0.596701801,\n",
       "  -0.141598403,\n",
       "  0.440042049,\n",
       "  0.387851566,\n",
       "  -0.612558722,\n",
       "  0.344112307,\n",
       "  -0.0537147,\n",
       "  -0.46595481,\n",
       "  -0.283597469,\n",
       "  0.11939574,\n",
       "  -0.201237619,\n",
       "  0.203903049,\n",
       "  -0.375532895,\n",
       "  -0.27020371,\n",
       "  0.156597123,\n",
       "  0.157281071,\n",
       "  -0.250227481,\n",
       "  -0.179565817,\n",
       "  0.0832151175,\n",
       "  -0.418110102,\n",
       "  0.751608491,\n",
       "  -0.259043574,\n",
       "  -0.101647414,\n",
       "  -0.0140705071,\n",
       "  0.495382369,\n",
       "  0.0983320475,\n",
       "  -0.18000868,\n",
       "  0.247827858,\n",
       "  0.0271476246,\n",
       "  -0.262985647,\n",
       "  -0.139576316,\n",
       "  -0.17529051,\n",
       "  -0.48690334,\n",
       "  -0.363838047,\n",
       "  -0.0803812072,\n",
       "  -0.0247186422]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER = 627.0 #pick anyone 0-100k to see watch history and recommendations\n",
    "NUM_NEIGH=3\n",
    "\n",
    "emb_627 = endpoint.predict([[USER]]) #prediction from the saved model\n",
    "emb_627 = emb_627.predictions[0]\n",
    "emb_627 # we should get our user xxx embedding @ dim len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a974e272-28e6-4b9f-bf75-62bd41ce50bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[MatchNeighbor(id='1478.0', distance=4.894585609436035),\n",
       "  MatchNeighbor(id='942.0', distance=4.2381672859191895),\n",
       "  MatchNeighbor(id='1135.0', distance=4.2109694480896),\n",
       "  MatchNeighbor(id='1004.0', distance=4.126661777496338),\n",
       "  MatchNeighbor(id='1136.0', distance=3.840146064758301),\n",
       "  MatchNeighbor(id='1267.0', distance=3.72882080078125),\n",
       "  MatchNeighbor(id='809.0', distance=3.7181546688079834),\n",
       "  MatchNeighbor(id='720.0', distance=3.669264554977417),\n",
       "  MatchNeighbor(id='157.0', distance=3.653459072113037),\n",
       "  MatchNeighbor(id='693.0', distance=3.645333766937256)]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ME_index_endpoint.match(queries=emb_627, deployed_index_id=DEPLOYED_INDEX_ID, num_neighbors=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f276c5-2899-4623-959c-9fadeda97cba",
   "metadata": {},
   "source": [
    "#### Create movie lookup tables\n",
    "Get what given user has rated highly, and what is being recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bade5a50-4525-405a-ae40-ec4fa06e6b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-09 02:12:36--  https://files.grouplens.org/datasets/movielens/ml-100k/u.item\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 236344 (231K)\n",
      "Saving to: ‘u.item.1’\n",
      "\n",
      "u.item.1            100%[===================>] 230.80K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2022-09-09 02:12:36 (3.02 MB/s) - ‘u.item.1’ saved [236344/236344]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://files.grouplens.org/datasets/movielens/ml-100k/u.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7621c74c-64d4-4d73-a083-53ee4c9a6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sidetour - create movie lookup dictionary\n",
    "movie_names = pd.read_csv('u.item', delimiter='|' , \n",
    "                          encoding='latin-1', \n",
    "                          usecols=(0,1),\n",
    "                          names = ['movie_id', 'title'])\n",
    "movielookup = movie_names.to_dict()['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c25fa63d-83d6-44d1-a60e-c67291771473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies watched: \n",
      " \n",
      "              0: Piano, The (1993)\n",
      "Movies watched: \n",
      " \n",
      "              1: Star Trek: The Wrath of Khan (1982)\n",
      "Movies watched: \n",
      " \n",
      "              2: Return of the Jedi (1983)\n",
      "Movies watched: \n",
      " \n",
      "              3: Star Trek VI: The Undiscovered Country (1991)\n",
      "Movies watched: \n",
      " \n",
      "              4: Star Trek III: The Search for Spock (1984)\n",
      "Movies watched: \n",
      " \n",
      "              5: Four Rooms (1995)\n",
      "Movies watched: \n",
      " \n",
      "              6: Addams Family Values (1993)\n",
      "Movies watched: \n",
      " \n",
      "              7: Arsenic and Old Lace (1944)\n",
      "Movies watched: \n",
      " \n",
      "              8: Pinocchio (1940)\n",
      "Movies watched: \n",
      " \n",
      "              9: Dead Poets Society (1989)\n"
     ]
    }
   ],
   "source": [
    "for i, watched_movie in enumerate(ratings.filter(lambda x: x['user_id']==USER)):\n",
    "    if i >= 10: #limit to top n\n",
    "        break\n",
    "    else:\n",
    "        key = watched_movie['movie_id'].numpy()\n",
    "        print(f\"\"\"Movies watched: \\n \n",
    "              {i}: {movielookup[key]}\"\"\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4a7cb350-eb73-4e0a-aa9f-18afea2f32b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movie IDs: [[MatchNeighbor(id='1478.0', distance=4.894585609436035), MatchNeighbor(id='942.0', distance=4.2381672859191895), MatchNeighbor(id='1135.0', distance=4.2109694480896)]]\n"
     ]
    }
   ],
   "source": [
    "query_vector = emb_627\n",
    "\n",
    "\n",
    "ann_response = ME_index_endpoint.match(\n",
    "    deployed_index_id='movielens_deployed2', \n",
    "    queries=query_vector, \n",
    "    num_neighbors=NUM_NEIGH\n",
    ")\n",
    "\n",
    "print(\"Recommended movie IDs:\", ann_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b02d9342-5e01-470a-810f-4a862d7a9608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies recommended: \n",
      " \n",
      "          0: Reckless (1995) (distance: 4.894585609436035)\n",
      "Movies recommended: \n",
      " \n",
      "          1: Killing Zoe (1994) (distance: 4.2381672859191895)\n",
      "Movies recommended: \n",
      " \n",
      "          2: Ghosts of Mississippi (1996) (distance: 4.2109694480896)\n"
     ]
    }
   ],
   "source": [
    "# look at the recommended movies vs the viewed for that user\n",
    "for i, match in enumerate(ann_response[0]):\n",
    "    key = int(float(match.id))\n",
    "    print(f\"\"\"Movies recommended: \\n \n",
    "          {i}: {movielookup[key]} (distance: {match.distance})\"\"\"\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e791683-6620-487d-bb2f-b783f6997886",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the tutorial. You can also manually delete resources that you created by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac66d7-b56c-4224-b847-fb29eb4fa045",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_RESOURCE_NAME\n",
    "# 7352179168240467968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f51e78-943e-43ae-8752-01843d5dd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61ecdd-df77-4939-ab93-2ed43dc8878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint_client.undeploy_index(index_endpoint=INDEX_ENDPOINT_NAME, deployed_index_id=DEPLOYED_INDEX_ID)\n",
    "\n",
    "index_client.delete_index(name=INDEX_RESOURCE_NAME)\n",
    "\n",
    "index_endpoint_client.delete_index_endpoint(name=INDEX_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037d03b-dfe6-43de-b2a7-ad01b3d4bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_resource_name = endpoint.resource_name\n",
    "endpoint_resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae4524-b080-486c-9a9f-16f5f9c21c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_resource_name = deployment.resource_name\n",
    "deployment_resource_name\n",
    "aiplatform.Endpoint.delete(endpoint, gcp_model)\n",
    "#delete our model endpoints, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41544df3-2341-439f-86a6-533826c6bf96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

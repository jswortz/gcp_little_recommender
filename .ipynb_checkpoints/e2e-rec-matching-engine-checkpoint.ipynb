{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3759d02-de65-4632-8a32-5261530d9c35",
   "metadata": {},
   "source": [
    "# E2E recsys with matching engine and TFRS\n",
    "\n",
    "\n",
    "Simple example, goal being:\n",
    "\n",
    "    1) Train a Two-Tower model using movielens data\n",
    "    \n",
    "    2) Deploy the query model endpoint\n",
    "    \n",
    "    3) Save movie embeddings to json, for use in matching engine\n",
    "    \n",
    "    \n",
    "#### Note on VPC Pairing - insturctions for in-notebook pairing [here](console.cloud.google.com/networking/networks/list?authuser=4&project=qwiklabs-gcp-01-782d5edf8d15)\n",
    "    \n",
    "First we will create a user-managed notebook behind the already created peered VPC network used for Matching Engine. Select tensorflow enterprise 2.6 with a T4 GPU\n",
    "\n",
    "\n",
    "![](./imgs/create-workbench.png)\n",
    "\n",
    "\n",
    "##### Be sure to create the notebook in the peered network\n",
    "\n",
    "\n",
    "![](./imgs/network-create.png)\n",
    "\n",
    "    \n",
    "The next notebook will connect matching engine with the query endpoint for a simple recommender system\n",
    "\n",
    "Run the below pip install one time to install tensorflow-recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1643d1c-9361-4880-8d43-d363f68da0c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.19.1-py2.py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-recommenders==0.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: tensorflow>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.6.0) (2.8.4)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.6.0) (1.3.0)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (3.19.6)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.22.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.57.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.15.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.51.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform) (3.0.9)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.6.3)\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.5.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.2.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.28.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.1.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.8.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (14.0.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (65.5.1)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (22.12.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.21.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.38.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2022.12.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.4.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (5.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.11.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders==0.6.0) (3.2.2)\n",
      "Installing collected packages: google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.19.1 google-cloud-bigquery-2.34.4 google-cloud-resource-manager-1.6.3\n"
     ]
    }
   ],
   "source": [
    "# !echo Y | pip uninstall tensorflow\n",
    "!pip install google-cloud-aiplatform tensorflow-recommenders==0.6.0 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd3e5a-545d-4d7b-87de-cb6422d08285",
   "metadata": {},
   "source": [
    "### Important - restart the kernel after installing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54877358-e37c-4404-9ab2-ca7360f9ef49",
   "metadata": {},
   "source": [
    "# Train a 2 tower model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d39132d-f08e-4d2c-be54-75e5067e8b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 15:06:35.357776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 15:06:35.369725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 15:06:35.371416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 15:06:35.374453: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 15:06:35.374849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 15:06:35.376606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 15:06:35.378294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 15:06:36.051258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 15:06:36.053171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 15:06:36.054776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 15:06:36.056336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13598 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "79/79 [==============================] - 20s 166ms/step - factorized_top_k/top_1_categorical_accuracy: 8.7500e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0012 - factorized_top_k/top_10_categorical_accuracy: 0.0045 - factorized_top_k/top_50_categorical_accuracy: 0.0465 - factorized_top_k/top_100_categorical_accuracy: 0.1090 - loss: 7101.8207 - regularization_loss: 0.0000e+00 - total_loss: 7101.8207\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 16s 171ms/step - factorized_top_k/top_1_categorical_accuracy: 6.2500e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0051 - factorized_top_k/top_10_categorical_accuracy: 0.0150 - factorized_top_k/top_50_categorical_accuracy: 0.1114 - factorized_top_k/top_100_categorical_accuracy: 0.2206 - loss: 6584.6044 - regularization_loss: 0.0000e+00 - total_loss: 6584.6044\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 15s 167ms/step - factorized_top_k/top_1_categorical_accuracy: 5.2500e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0093 - factorized_top_k/top_10_categorical_accuracy: 0.0254 - factorized_top_k/top_50_categorical_accuracy: 0.1571 - factorized_top_k/top_100_categorical_accuracy: 0.2853 - loss: 6413.8690 - regularization_loss: 0.0000e+00 - total_loss: 6413.8690\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 15s 162ms/step - factorized_top_k/top_1_categorical_accuracy: 6.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0117 - factorized_top_k/top_10_categorical_accuracy: 0.0328 - factorized_top_k/top_50_categorical_accuracy: 0.1969 - factorized_top_k/top_100_categorical_accuracy: 0.3402 - loss: 6306.4593 - regularization_loss: 0.0000e+00 - total_loss: 6306.4593\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 16s 173ms/step - factorized_top_k/top_1_categorical_accuracy: 6.7500e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0128 - factorized_top_k/top_10_categorical_accuracy: 0.0362 - factorized_top_k/top_50_categorical_accuracy: 0.2236 - factorized_top_k/top_100_categorical_accuracy: 0.3769 - loss: 6226.1449 - regularization_loss: 0.0000e+00 - total_loss: 6226.1449\n",
      "20/20 [==============================] - 6s 173ms/step - factorized_top_k/top_1_categorical_accuracy: 5.0000e-05 - factorized_top_k/top_5_categorical_accuracy: 3.0000e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0016 - factorized_top_k/top_50_categorical_accuracy: 0.0555 - factorized_top_k/top_100_categorical_accuracy: 0.1505 - loss: 6878.6962 - regularization_loss: 0.0000e+00 - total_loss: 6878.6962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 4.999999873689376e-05,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.0003000000142492354,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0015999999595806003,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.05550000071525574,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.1505499929189682,\n",
       " 'loss': 3468.936279296875,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 3468.936279296875}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "\n",
    "from google.protobuf import struct_pb2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "DIMENSIONS = 64 # this is how large the embedding dimensions get\n",
    "\n",
    "\n",
    "# Ratings data.\n",
    "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
    "\n",
    "# Select the basic features.\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_id\": tf.strings.to_number(x[\"movie_id\"]),\n",
    "    \"user_id\": tf.strings.to_number(x[\"user_id\"])\n",
    "})\n",
    "movies = movies.map(lambda x: tf.strings.to_number(x[\"movie_id\"]))\n",
    "\n",
    "# Build a model.\n",
    "class Model(tfrs.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up user representation.\n",
    "        self.user_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(\n",
    "            input_dim=2000, output_dim=DIMENSIONS),\n",
    "            ])\n",
    "        # Set up movie representation.\n",
    "        self.item_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(\n",
    "            input_dim=2000, output_dim=DIMENSIONS),\n",
    "        ])\n",
    "        # Set up a retrieval task and evaluation metrics over the\n",
    "        # entire dataset of candidates.\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=movies.batch(128).map(self.item_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        movie_embeddings = self.item_model(features[\"movie_id\"])\n",
    "\n",
    "        return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# Randomly shuffle data and split between train and test.\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "# Train.\n",
    "model.fit(train.batch(1024), epochs=5)\n",
    "\n",
    "# Evaluate.\n",
    "model.evaluate(test.batch(1024), return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbaec34-8a04-404d-aaa7-e8fe0eb82797",
   "metadata": {},
   "source": [
    "### Set your variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243a44f-32d1-4c14-80f1-1c3a7cd96ba3",
   "metadata": {},
   "source": [
    "Run `!gcloud auth login` in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367695ef-4330-4dee-a296-94ac68850744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/matchine-engine/locations/us-central1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT = 'matchine-engine' #set to your own\n",
    "NETWORK_NAME = 'matching-engine-vpc' #same as VPC peered network\n",
    "\n",
    "### Create a bucket to store our embeddings and models\n",
    "BUCKET = 'gs://jwortz-bucket' # TODO - change for each user\n",
    "EMBEDDINGS = os.path.join(BUCKET, 'embeddings')\n",
    "QUERY_MODEL = os.path.join(BUCKET, 'query_model')\n",
    "REGION = 'us-central1'\n",
    "\n",
    "## Gets an auth token with the Parent variable\n",
    "PROJECT_ID = PROJECT\n",
    "AUTH_TOKEN = !gcloud auth print-access-token\n",
    "PROJECT_NUMBER = ! gcloud projects list --filter=\"$PROJECT\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
    "\n",
    "PARENT = \"projects/{}/locations/{}\".format(PROJECT_ID, REGION)\n",
    "PARENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c97eedb-0d0d-4eea-a90f-ed3fd162b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://jwortz-bucket/...\n"
     ]
    }
   ],
   "source": [
    "# run one time to create your bucket\n",
    "!gsutil mb -l $REGION $BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03c40d03-6ee1-4e50-8742-1455bcfaf632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the query/user model\n",
    "\n",
    "model.user_model.save(QUERY_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ac9bbe-d100-4db9-aa6a-97889b540ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jwortz-bucket/query_model/\n",
      "gs://jwortz-bucket/query_model/keras_metadata.pb\n",
      "gs://jwortz-bucket/query_model/saved_model.pb\n",
      "gs://jwortz-bucket/query_model/assets/\n",
      "gs://jwortz-bucket/query_model/variables/\n"
     ]
    }
   ],
   "source": [
    "# Make sure it saved\n",
    "!gsutil ls $QUERY_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14604762-46fd-413b-97fe-dd3c3e4ce182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "model_gcp = aiplatform.Model.upload(\n",
    "        display_name=\"Movielens User Query Model\",\n",
    "        artifact_uri=QUERY_MODEL,\n",
    "        serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest',\n",
    "        description=\"Top of the query tower, meant to return an embedding for each user instance\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f87b1b-5686-4640-bac9-aa2b4d836417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Model object at 0x7f25e814ec50> \n",
       "resource name: projects/807304860730/locations/us-central1/models/3397194061688340480"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate the model type output\n",
    "model_gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3f621f-60a6-47d3-b7a3-49082f6c9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a05ae9-6deb-4db3-91f2-41676067f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=\"Movielens Model Endpoint\",\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ddcf4a-c5a2-4593-9895-93b811081a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = model_gcp.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=\"Movielens User Query Model\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=2,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=0,\n",
    "    sync=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ed84db-5c12-44fe-ad93-580b9bf7a736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f26cacfec50> \n",
       "resource name: projects/807304860730/locations/us-central1/endpoints/6840374097696784384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb85c3-18b9-470d-9880-bd91b3d22564",
   "metadata": {},
   "source": [
    "## Save the embeddings for the movie dataset\n",
    "\n",
    "### Write embeddings to local storage\n",
    "Following this format for Matching Engine\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/matching_engine/sdk_matching_engine_for_indexing.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c49bab-4aa0-484c-84cf-ceebe88c5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_embs = movies.batch(1000).map(lambda x: [x, model.item_model(x)]).unbatch() #process 1000 at a time then flatten it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eed5978-7ee0-48e1-a98a-08119a52989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to local disk\n",
    "with open(\"movie_embeddings.json\", 'w') as f:\n",
    "    for movie_id, movie_emb in movie_embs:\n",
    "        # print(movie_id.numpy(), movie_emb.numpy())\n",
    "        f.write('{\"id\":\"' + str(movie_id.numpy()) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(movie_emb.numpy())) + ']}')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccbe63-9f63-4da0-8314-30244c7a8df1",
   "metadata": {},
   "source": [
    "You should now see .json data as required by matching engine\n",
    "![](imgs/jsonl.png)\n",
    "\n",
    "### Upload the data to GCS\n",
    "Only remove if you have issues uploading the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd0c7011-dbf2-4f73-adcc-89aff7ecb7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://movie_embeddings.json [Content-Type=application/json]...\n",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \n",
      "Operation completed over 1 objects/1.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp movie_embeddings.json $EMBEDDINGS/movie_embeddings.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1af7d-d6db-4bd6-91d6-f8343700ea94",
   "metadata": {},
   "source": [
    "# Next we will deploy our movie inidicies. With Matching Engine\n",
    "* Create an index (from the `json` files)\n",
    "* Create and endpoint\n",
    "* Deploy the index to the endpoint so you can perform vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccda516e-4365-48e0-ab58-aa93e3a733e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49483180-e16a-48ac-9954-557dc2718144",
   "metadata": {},
   "source": [
    "## Set the Nearest Neighbor Options\n",
    "\n",
    "See here for tips on [tuning the index](https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#tuning_the_index)\n",
    "\n",
    "From the paper - here's the rough idea\n",
    "\n",
    "1. (Initialization Step) Select a dictionary C(m) bysampling from {x(m) 1 , . . . x (m) n }. \n",
    "2. (Partition Assignment Step) For each datapoint xi , update x˜i by using the value of c ∈ C (m) that minimizes the anisotropic loss of ˜xi.\n",
    "3. (Codebook Update Step) Optimize the loss function over all codewords in all dictionaries while keeping every dictionaries partitions constant.\n",
    "4. Repeat Step 2 and Step 3 until convergence to a fixed point or maximum number of iteration is reached.\n",
    "\n",
    "\n",
    "### Relating the algorithm to the parameters:\n",
    "\n",
    "* `leafNodeEmbeddingCount` -> Number of embeddings on each leaf node. The default value is 1000 if not set.\n",
    "* `leafNodesToSearchPercent` -> The default percentage of leaf nodes that any query may be searched. Must be in range 1-100, inclusive. The default value is 10 (means 10%) if not set.\n",
    "* `approximateNeighborsCount` -> The default number of neighbors to find through approximate search before exact reordering is performed. Exact reordering is a procedure where results returned by an approximate search algorithm are reordered via a more expensive distance computation.\n",
    "* `distanceMeasureType` -> DOT_PRODUCT_DISTANCE is default - COSINE, L1 and L2^2 is available\n",
    "\n",
    "Other best practices from our PM team:\n",
    "```\n",
    "Start from leafNodesToSearchPercent=5 and approximateNeighborsCount=10 * k\n",
    "\n",
    "use default values for others.\n",
    "\n",
    "measure performance and recall and change those 2 parameters accordingly.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "994a7ccd-845d-415c-b106-ad6950700415",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"me-index-endpoint-movielens-demo\"\n",
    "\n",
    "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    contents_delta_uri=EMBEDDINGS,\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=7,\n",
    "    description=\"Movielens Demo2\",\n",
    "    labels={\"label_name\": \"label_value\"},\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea507c13-1434-4e7c-88ae-d8fd0e6e7665",
   "metadata": {},
   "source": [
    "## Note on the advantages of the algorithm\n",
    "\n",
    "[link](https://arxiv.org/pdf/1908.10396.pdf)\n",
    "\n",
    "```However, it is easy to see that not all pairs of (x, q) are equally important. The approximation error on the pairs which have a high inner product is far more important since they are likely to be among the top ranked pairs and can greatly affect the search result, while for the pairs whose inner product is low the approximation error matters much less. In other words, for a given datapoint x, we should quantize it with a bigger focus on its error with those queries which have high inner product with x. See Figure 1 for the illustration.```\n",
    "\n",
    "\n",
    "![](imgs/algo.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574de1e3-d21c-4d56-ab24-41f052da9516",
   "metadata": {},
   "source": [
    "### Use existing to skip creation time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71531766-8950-4d82-8e9c-5d2aff43c1c0",
   "metadata": {},
   "source": [
    "```python\n",
    "tree_ah_index = aiplatform.MatchingEngineIndex('6576460520704966656')\n",
    "tree_ah_index\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af30e4-6572-4843-8539-f9801bf89609",
   "metadata": {},
   "source": [
    "### Save the name of the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34c7bcbe-6bc5-4740-ae7d-a7bc40389ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MatchingEngineIndex' object has no attribute 'result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24399/2351287117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mINDEX_RESOURCE_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_ah_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mINDEX_RESOURCE_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MatchingEngineIndex' object has no attribute 'result'"
     ]
    }
   ],
   "source": [
    "INDEX_RESOURCE_NAME = tree_ah_index.result().name\n",
    "INDEX_RESOURCE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bab2c7-c7ca-45d6-ae49-00903d13d0a7",
   "metadata": {},
   "source": [
    "Debugging tool in case you run into issues. Example usage below.\n",
    "`!gcloud beta ai operations describe 4122851463774863360 --index=7253099976438317056 --project=$PROJECT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae2c000-792b-4766-9562-fddf93387308",
   "metadata": {},
   "source": [
    "## Create Index Endpoint and Deploy Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596eea7a-c339-45ee-9443-e4777e073bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/807304860730/global/networks/matching-engine-vpc'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VPC_NETWORK_NAME = \"projects/{}/global/networks/{}\".format(PROJECT_NUMBER, NETWORK_NAME)\n",
    "VPC_NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "840dfb0e-a653-4fae-b67d-d1ff42a2a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=\"index_endpoint_for_demo\",\n",
    "    description=\"endpoint for movielens\",\n",
    "    network=VPC_NETWORK_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95f6267d-e920-475a-899e-25811f4eefbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/807304860730/locations/us-central1/indexEndpoints/8211267185440456704'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_ENDPOINT_NAME = my_index_endpoint.resource_name\n",
    "INDEX_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590a6f5-addf-4d8e-9ad0-88ec740975c9",
   "metadata": {},
   "source": [
    "### Deploy the index to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b105a5f-f56d-4d20-af61-8c3590a89a61",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "Operation did not complete within the designated timeout of 900 seconds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_OperationNotComplete\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_done_or_raise\u001b[0;34m(self, retry)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_OperationNotComplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_OperationNotComplete\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_blocking_poll\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mpolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_or_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0mlast_exc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 ) from last_exc\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRetryError\u001b[0m: Deadline of 900.0s exceeded while calling target function, last exception: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24399/2739928124.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m my_index_endpoint = my_index_endpoint.deploy_index(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_ah_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeployed_index_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEPLOYED_INDEX_ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/matching_engine/matching_engine_index_endpoint.py\u001b[0m in \u001b[0;36mdeploy_index\u001b[0;34m(self, index, deployed_index_id, display_name, machine_type, min_replica_count, max_replica_count, enable_access_logging, reserved_ip_ranges, deployment_group, auth_config_audiences, auth_config_allowed_issuers, request_metadata)\u001b[0m\n\u001b[1;32m    678\u001b[0m         )\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mdeploy_lro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         _LOGGER.log_action_completed_against_resource(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \"\"\"\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_blocking_poll\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             raise concurrent.futures.TimeoutError(\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0;34mf\"Operation did not complete within the designated timeout of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0;34mf\"{polling.timeout} seconds.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             )\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Operation did not complete within the designated timeout of 900 seconds."
     ]
    }
   ],
   "source": [
    "DEPLOYED_INDEX_ID = \"tree_movielens\" #change to unique id or use this one\n",
    "\n",
    "## note you can comment out if you are not creating your own index endpoint\n",
    "\n",
    "my_index_endpoint = my_index_endpoint.deploy_index(\n",
    "    index=tree_ah_index, deployed_index_id=DEPLOYED_INDEX_ID\n",
    ")\n",
    "\n",
    "my_index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989d6a5-f676-402c-bda2-7f12421a8a62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other quick notes on ME while we wait for deployment\n",
    "\n",
    "[link](https://cloud.google.com/blog/topics/developers-practitioners/find-anything-blazingly-fast-googles-vector-search-technology)\n",
    "\n",
    "Instead of comparing vectors one by one, you could use the approximate nearest neighbor (ANN) approach to improve search times. Many ANN algorithms use vector quantization (VQ), in which you split the vector space into multiple groups, define \"codewords\" to represent each group, and search only for those codewords. This VQ technique dramatically enhances query speeds and is the essential part of many ANN algorithms, just like indexing is the essential part of relational databases and full-text search engines.\n",
    "\n",
    "![](imgs/vectorQuant.gif)\n",
    "\n",
    "\n",
    "As you may be able to conclude from the diagram above, as the number of groups in the space increases the speed of the search decreases and the accuracy increases.  Managing this trade-off — getting higher accuracy at shorter latency — has been a key challenge with ANN algorithms. \n",
    "\n",
    "Last year, Google Research announced ScaNN, a new solution that provides state-of-the-art results for this challenge. With ScaNN, they introduced a new VQ algorithm called anisotropic vector quantization:\n",
    "\n",
    "![](imgs/Loss_Types.max-1000x1000.png)\n",
    "\n",
    "Anisotropic vector quantization uses a new loss function to train a model for VQ for an optimal grouping to capture farther data points (i.e. higher inner product) in a single group. With this idea, the new algorithm gives you higher accuracy at lower latency, as you can see in the benchmark result below (the violet line): \n",
    "\n",
    "![](imgs/speedvsaccuracy.max-1600x1600.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc5328-aee5-4042-ba33-95604e74e196",
   "metadata": {},
   "source": [
    "# Connect Matching Engine and The User Model Into a Recommendation System\n",
    "\n",
    "This will bring it all together by incorporating the prediction endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3663265-9097-4ab5-8ed5-b6065ceecec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish index_endpoint -IMPORTANT for constructing already created endpoints/indicies/etc...\n",
    "ME_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(INDEX_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78b86481-e4fa-4c9c-b37d-8ab691f08574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.162882209,\n",
       "  -0.335985959,\n",
       "  0.137335211,\n",
       "  0.0739337727,\n",
       "  -0.432961673,\n",
       "  -0.394063562,\n",
       "  0.34763962,\n",
       "  -0.080462493,\n",
       "  -0.424611747,\n",
       "  0.217125714,\n",
       "  -0.45972839,\n",
       "  0.494097382,\n",
       "  0.298708528,\n",
       "  -0.17870906,\n",
       "  0.617574215,\n",
       "  -0.556233346,\n",
       "  -0.364824772,\n",
       "  0.218952522,\n",
       "  0.604446352,\n",
       "  -0.316697538,\n",
       "  -0.873303175,\n",
       "  -0.162815481,\n",
       "  0.500744939,\n",
       "  0.257559836,\n",
       "  -0.100139424,\n",
       "  0.667596281,\n",
       "  0.108864874,\n",
       "  0.378669232,\n",
       "  -0.337707669,\n",
       "  0.0600555874,\n",
       "  -0.129736587,\n",
       "  -0.336009651,\n",
       "  -0.0422365814,\n",
       "  -0.399091274,\n",
       "  0.267339408,\n",
       "  0.0344658494,\n",
       "  0.353300542,\n",
       "  -0.0192649253,\n",
       "  0.0923104733,\n",
       "  -0.102977142,\n",
       "  0.126158774,\n",
       "  0.190066129,\n",
       "  -0.167369932,\n",
       "  0.252656192,\n",
       "  0.048801031,\n",
       "  0.340473771,\n",
       "  -0.397580266,\n",
       "  -0.169092372,\n",
       "  0.0312889926,\n",
       "  0.424314827,\n",
       "  0.298339814,\n",
       "  -0.315201908,\n",
       "  0.464916021,\n",
       "  0.0904096663,\n",
       "  0.603862882,\n",
       "  -0.147781983,\n",
       "  0.492935568,\n",
       "  -0.571237922,\n",
       "  0.306813419,\n",
       "  0.0781588256,\n",
       "  0.566045702,\n",
       "  0.197490379,\n",
       "  -0.198064387,\n",
       "  -0.410030544]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER = 627.0 #pick anyone 0-100k to see watch history and recommendations\n",
    "NUM_NEIGH=3\n",
    "\n",
    "emb_627 = endpoint.predict([[USER]]) #prediction from the saved model\n",
    "emb_627 = emb_627.predictions[0]\n",
    "emb_627 # we should get our user xxx embedding @ dim len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a974e272-28e6-4b9f-bf75-62bd41ce50bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[MatchNeighbor(id='394.0', distance=5.03569221496582),\n",
       "  MatchNeighbor(id='1075.0', distance=4.747988700866699),\n",
       "  MatchNeighbor(id='1299.0', distance=4.696071624755859),\n",
       "  MatchNeighbor(id='1410.0', distance=4.3028106689453125),\n",
       "  MatchNeighbor(id='1311.0', distance=4.258786201477051),\n",
       "  MatchNeighbor(id='725.0', distance=4.23414945602417),\n",
       "  MatchNeighbor(id='1508.0', distance=4.20390510559082),\n",
       "  MatchNeighbor(id='542.0', distance=4.15582275390625),\n",
       "  MatchNeighbor(id='1538.0', distance=4.070896148681641),\n",
       "  MatchNeighbor(id='1531.0', distance=4.002253532409668)]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ME_index_endpoint.match(queries=emb_627, deployed_index_id=DEPLOYED_INDEX_ID, num_neighbors=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f276c5-2899-4623-959c-9fadeda97cba",
   "metadata": {},
   "source": [
    "#### Create movie lookup tables\n",
    "Get what given user has rated highly, and what is being recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bade5a50-4525-405a-ae40-ec4fa06e6b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-12 16:36:48--  https://files.grouplens.org/datasets/movielens/ml-100k/u.item\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 236344 (231K)\n",
      "Saving to: ‘u.item.2’\n",
      "\n",
      "u.item.2            100%[===================>] 230.80K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2022-12-12 16:36:48 (3.06 MB/s) - ‘u.item.2’ saved [236344/236344]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://files.grouplens.org/datasets/movielens/ml-100k/u.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7621c74c-64d4-4d73-a083-53ee4c9a6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sidetour - create movie lookup dictionary\n",
    "movie_names = pd.read_csv('u.item', delimiter='|' , \n",
    "                          encoding='latin-1', \n",
    "                          usecols=(0,1),\n",
    "                          names = ['movie_id', 'title'])\n",
    "movielookup = movie_names.to_dict()['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c25fa63d-83d6-44d1-a60e-c67291771473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Watched:\n",
      "\n",
      " \n",
      "              0: Piano, The (1993)\n",
      "\n",
      " \n",
      "              1: Star Trek: The Wrath of Khan (1982)\n",
      "\n",
      " \n",
      "              2: Return of the Jedi (1983)\n",
      "\n",
      " \n",
      "              3: Star Trek VI: The Undiscovered Country (1991)\n",
      "\n",
      " \n",
      "              4: Star Trek III: The Search for Spock (1984)\n",
      "\n",
      " \n",
      "              5: Four Rooms (1995)\n",
      "\n",
      " \n",
      "              6: Addams Family Values (1993)\n",
      "\n",
      " \n",
      "              7: Arsenic and Old Lace (1944)\n",
      "\n",
      " \n",
      "              8: Pinocchio (1940)\n",
      "\n",
      " \n",
      "              9: Dead Poets Society (1989)\n"
     ]
    }
   ],
   "source": [
    "print(\"Movies Watched:\")\n",
    "for i, watched_movie in enumerate(ratings.filter(lambda x: x['user_id']==USER)):\n",
    "    if i >= 10: #limit to top n\n",
    "        break\n",
    "    else:\n",
    "        key = watched_movie['movie_id'].numpy()\n",
    "        print(f\"\"\"\\n \n",
    "              {i}: {movielookup[key]}\"\"\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a7cb350-eb73-4e0a-aa9f-18afea2f32b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movie IDs: [[MatchNeighbor(id='394.0', distance=5.03569221496582), MatchNeighbor(id='1075.0', distance=4.747988700866699), MatchNeighbor(id='1299.0', distance=4.696071624755859)]]\n"
     ]
    }
   ],
   "source": [
    "query_vector = emb_627\n",
    "\n",
    "\n",
    "ann_response = ME_index_endpoint.match(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID, \n",
    "    queries=query_vector, \n",
    "    num_neighbors=NUM_NEIGH\n",
    ")\n",
    "\n",
    "print(\"Recommended movie IDs:\", ann_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b02d9342-5e01-470a-810f-4a862d7a9608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies recommended: \n",
      "\n",
      " \n",
      "          0: Robin Hood: Men in Tights (1993) (distance: 5.03569221496582)\n",
      "\n",
      " \n",
      "          1: Pagemaster, The (1994) (distance: 4.747988700866699)\n",
      "\n",
      " \n",
      "          2: 'Til There Was You (1997) (distance: 4.696071624755859)\n"
     ]
    }
   ],
   "source": [
    "# look at the recommended movies vs the viewed for that user\n",
    "print(\"Movies recommended: \")\n",
    "for i, match in enumerate(ann_response[0]):\n",
    "    key = int(float(match.id))\n",
    "    print(f\"\"\"\\n \n",
    "          {i}: {movielookup[key]} (distance: {match.distance})\"\"\"\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49afb92-d1c7-4a06-a3d1-31445df7fda1",
   "metadata": {},
   "source": [
    "### Bonus topic\n",
    "\n",
    "Streaming upserts and compaction details can be found on the official guide [here](https://cloud.google.com/vertex-ai/docs/matching-engine/update-rebuild-index#update_an_index_using_streaming_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e791683-6620-487d-bb2f-b783f6997886",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the tutorial. You can also manually delete resources that you created by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac66d7-b56c-4224-b847-fb29eb4fa045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force undeployment of indexes and delete endpoint\n",
    "my_index_endpoint.delete(force=True)\n",
    "tree_ah_index.delete()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m95"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "local-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

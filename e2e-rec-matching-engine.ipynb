{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3759d02-de65-4632-8a32-5261530d9c35",
   "metadata": {},
   "source": [
    "# E2E recsys with matching engine and TFRS\n",
    "\n",
    "\n",
    "Simple example, goal being:\n",
    "\n",
    "    1) Train a Two-Tower model using movielens data\n",
    "    \n",
    "    2) Deploy the query model endpoint\n",
    "    \n",
    "    3) Save movie embeddings to json, for use in matching engine\n",
    "    \n",
    "    \n",
    "#### Note on VPC Pairing - insturctions for in-notebook pairing [here](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/matching_engine/sdk_matching_engine_for_indexing.ipynb)\n",
    "    \n",
    "First we will create a user-managed notebook behind the already created peered VPC network used for Matching Engine. Select tensorflow enterprise 2.6 with a T4 GPU\n",
    "\n",
    "\n",
    "![](./create-workbench.png)\n",
    "\n",
    "\n",
    "##### Be sure to create the notebook in the peered network\n",
    "\n",
    "\n",
    "![](./network-create.png)\n",
    "\n",
    "    \n",
    "The next notebook will connect matching engine with the query endpoint for a simple recommender system\n",
    "\n",
    "Run the below pip install one time to install tensorflow-recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1643d1c-9361-4880-8d43-d363f68da0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.9.1\n",
      "Uninstalling tensorflow-2.9.1:\n",
      "  Would remove:\n",
      "    /home/jupyter/.local/lib/python3.7/site-packages/tensorflow-2.9.1.dist-info/*\n",
      "    /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/*\n",
      "Proceed (Y/n)?   Successfully uninstalled tensorflow-2.9.1\n",
      "Collecting tensorflow-recommenders\n",
      "  Using cached tensorflow_recommenders-0.7.0-py3-none-any.whl (88 kB)\n",
      "Collecting tensorflow>=2.9.0\n",
      "  Using cached tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow-recommenders) (1.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.47.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.21.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (59.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (3.10.0.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.12.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (0.4.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (2.9.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (3.19.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (21.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (0.26.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (2.9.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (14.0.6)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.37.1)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow>=2.9.0->tensorflow-recommenders) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (3.3.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (2.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow>=2.9.0->tensorflow-recommenders) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (4.11.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (2.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.9.0->tensorflow-recommenders) (3.2.0)\n",
      "Installing collected packages: tensorflow, tensorflow-recommenders\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx-bsl 1.9.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.52.0 which is incompatible.\n",
      "tfx-bsl 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\n",
      "tensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.9.1 which is incompatible.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tensorflow-2.9.1 tensorflow-recommenders-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!echo Y | pip uninstall tensorflow\n",
    "!pip install tensorflow-recommenders --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd3e5a-545d-4d7b-87de-cb6422d08285",
   "metadata": {},
   "source": [
    "### Important - restart the kernel after installing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54877358-e37c-4404-9ab2-ca7360f9ef49",
   "metadata": {},
   "source": [
    "# Train a 2 tower model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d39132d-f08e-4d2c-be54-75e5067e8b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 4.70 MiB (download: 4.70 MiB, generated: 32.41 MiB, total: 37.10 MiB) to /home/jupyter/tensorflow_datasets/movielens/100k-ratings/0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f14a10959974ae1b192ae228b08f76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ce42f31ddd4c0fa8d15973c87d286e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3fdba24bb84f8499ece1442a9b43ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling movielens-train.tfrecord...:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to /home/jupyter/tensorflow_datasets/movielens/100k-ratings/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 18:50:04.004307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 18:50:04.078415: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-08-08 18:50:04.089592: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-08-08 18:50:04.091144: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 4.70 MiB (download: 4.70 MiB, generated: 150.35 KiB, total: 4.84 MiB) to /home/jupyter/tensorflow_datasets/movielens/100k-movies/0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa334f9890743a8aaf3dc5bd20b1024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcf29b65dd64d19b5ce2871e4b439e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42a6caf7ddc4ebd9f98063c3735547a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/1682 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling movielens-train.tfrecord...:   0%|          | 0/1682 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to /home/jupyter/tensorflow_datasets/movielens/100k-movies/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 18s 158ms/step - factorized_top_k/top_1_categorical_accuracy: 5.0000e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0012 - factorized_top_k/top_10_categorical_accuracy: 0.0042 - factorized_top_k/top_50_categorical_accuracy: 0.0462 - factorized_top_k/top_100_categorical_accuracy: 0.1077 - loss: 7099.8470 - regularization_loss: 0.0000e+00 - total_loss: 7099.8470\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 15s 163ms/step - factorized_top_k/top_1_categorical_accuracy: 1.1250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0051 - factorized_top_k/top_10_categorical_accuracy: 0.0155 - factorized_top_k/top_50_categorical_accuracy: 0.1111 - factorized_top_k/top_100_categorical_accuracy: 0.2182 - loss: 6582.0584 - regularization_loss: 0.0000e+00 - total_loss: 6582.0584\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 14s 157ms/step - factorized_top_k/top_1_categorical_accuracy: 6.0000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0098 - factorized_top_k/top_10_categorical_accuracy: 0.0252 - factorized_top_k/top_50_categorical_accuracy: 0.1584 - factorized_top_k/top_100_categorical_accuracy: 0.2850 - loss: 6412.9182 - regularization_loss: 0.0000e+00 - total_loss: 6412.9182\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 14s 157ms/step - factorized_top_k/top_1_categorical_accuracy: 6.2500e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0123 - factorized_top_k/top_10_categorical_accuracy: 0.0324 - factorized_top_k/top_50_categorical_accuracy: 0.1972 - factorized_top_k/top_100_categorical_accuracy: 0.3399 - loss: 6305.4666 - regularization_loss: 0.0000e+00 - total_loss: 6305.4666\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 14s 156ms/step - factorized_top_k/top_1_categorical_accuracy: 6.8750e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0135 - factorized_top_k/top_10_categorical_accuracy: 0.0358 - factorized_top_k/top_50_categorical_accuracy: 0.2228 - factorized_top_k/top_100_categorical_accuracy: 0.3758 - loss: 6225.6934 - regularization_loss: 0.0000e+00 - total_loss: 6225.6934\n",
      "20/20 [==============================] - 6s 158ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 1.0000e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0016 - factorized_top_k/top_50_categorical_accuracy: 0.0534 - factorized_top_k/top_100_categorical_accuracy: 0.1502 - loss: 6881.1233 - regularization_loss: 0.0000e+00 - total_loss: 6881.1233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 9.999999747378752e-05,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0016499999910593033,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.053449999541044235,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.1501999944448471,\n",
       " 'loss': 3467.974609375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 3467.974609375}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "\n",
    "from google.cloud import aiplatform_v1beta1 #needed for matching engine calls\n",
    "from google.protobuf import struct_pb2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "DIMENSIONS = 64 # this is how large the embedding dimensions get\n",
    "\n",
    "\n",
    "# Ratings data.\n",
    "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
    "\n",
    "# Select the basic features.\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_id\": tf.strings.to_number(x[\"movie_id\"]),\n",
    "    \"user_id\": tf.strings.to_number(x[\"user_id\"])\n",
    "})\n",
    "movies = movies.map(lambda x: tf.strings.to_number(x[\"movie_id\"]))\n",
    "\n",
    "# Build a model.\n",
    "class Model(tfrs.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up user representation.\n",
    "        self.user_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(\n",
    "            input_dim=2000, output_dim=DIMENSIONS),\n",
    "            ])\n",
    "        # Set up movie representation.\n",
    "        self.item_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(\n",
    "            input_dim=2000, output_dim=DIMENSIONS),\n",
    "        ])\n",
    "        # Set up a retrieval task and evaluation metrics over the\n",
    "        # entire dataset of candidates.\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=movies.batch(128).map(self.item_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        movie_embeddings = self.item_model(features[\"movie_id\"])\n",
    "\n",
    "        return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# Randomly shuffle data and split between train and test.\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "# Train.\n",
    "model.fit(train.batch(1024), epochs=5)\n",
    "\n",
    "# Evaluate.\n",
    "model.evaluate(test.batch(1024), return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbaec34-8a04-404d-aaa7-e8fe0eb82797",
   "metadata": {},
   "source": [
    "### Set your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367695ef-4330-4dee-a296-94ac68850744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/wortz-project-352116/locations/us-central1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT = 'wortz-project-352116' #set to your own\n",
    "NETWORK_NAME = 'me-network' #same as VPC peered network\n",
    "\n",
    "### Create a bucket to store our embeddings and models\n",
    "BUCKET = 'gs://end-to-end-two-tower' # TODO - change for each user\n",
    "EMBEDDINGS = os.path.join(BUCKET, 'embeddings')\n",
    "QUERY_MODEL = os.path.join(BUCKET, 'query_model')\n",
    "REGION = 'us-central1'\n",
    "\n",
    "## Gets an auth token with the Parent variable\n",
    "PROJECT_ID = PROJECT\n",
    "AUTH_TOKEN = !gcloud auth print-access-token\n",
    "PROJECT_NUMBER = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
    "\n",
    "\n",
    "PARENT = \"projects/{}/locations/{}\".format(PROJECT_ID, REGION)\n",
    "PARENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c97eedb-0d0d-4eea-a90f-ed3fd162b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run one time to create your bucket\n",
    "# !gsutil mb -l $REGION $BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c40d03-6ee1-4e50-8742-1455bcfaf632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the query/user model\n",
    "\n",
    "model.user_model.save(QUERY_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ac9bbe-d100-4db9-aa6a-97889b540ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://end-to-end-two-tower/query_model/\n",
      "gs://end-to-end-two-tower/query_model/keras_metadata.pb\n",
      "gs://end-to-end-two-tower/query_model/saved_model.pb\n",
      "gs://end-to-end-two-tower/query_model/assets/\n",
      "gs://end-to-end-two-tower/query_model/variables/\n"
     ]
    }
   ],
   "source": [
    "# Make sure it saved\n",
    "!gsutil ls $QUERY_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14604762-46fd-413b-97fe-dd3c3e4ce182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "model_gcp = aiplatform.Model.upload(\n",
    "        display_name=\"Movielens User Query Model\",\n",
    "        artifact_uri=QUERY_MODEL,\n",
    "        serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest',\n",
    "        description=\"Top of the query tower, meant to return an embedding for each user instance\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f87b1b-5686-4640-bac9-aa2b4d836417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Model object at 0x7f1557005f10> \n",
       "resource name: projects/679926387543/locations/us-central1/models/3782685037409861632"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate the model type output\n",
    "model_gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f621f-60a6-47d3-b7a3-49082f6c9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a05ae9-6deb-4db3-91f2-41676067f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=\"Movielens Model Endpoint\",\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ddcf4a-c5a2-4593-9895-93b811081a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "deployment = model_gcp.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=\"Movielens User Query Model\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=2,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=0,\n",
    "    sync=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4ed84db-5c12-44fe-ad93-580b9bf7a736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f15c7691a90> \n",
       "resource name: projects/679926387543/locations/us-central1/endpoints/2924023630721449984"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/proto/marshal/rules/enums.py:40: UserWarning: Unrecognized DeploymentResourcesType enum value: 3\n",
      "  value=value,\n"
     ]
    }
   ],
   "source": [
    "deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb85c3-18b9-470d-9880-bd91b3d22564",
   "metadata": {},
   "source": [
    "## Save the embeddings for the movie dataset\n",
    "\n",
    "### Write embeddings to local storage\n",
    "Following this format for Matching Engine\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/matching_engine/sdk_matching_engine_for_indexing.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c49bab-4aa0-484c-84cf-ceebe88c5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_embs = movies.batch(1000).map(lambda x: [x, model.item_model(x)]).unbatch() #process 1000 at a time then flatten it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eed5978-7ee0-48e1-a98a-08119a52989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to local disk\n",
    "with open(\"movie_embeddings.json\", 'w') as f:\n",
    "    for movie_id, movie_emb in movie_embs:\n",
    "        # print(movie_id.numpy(), movie_emb.numpy())\n",
    "        f.write('{\"id\":\"' + str(movie_id.numpy()) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(movie_emb.numpy())) + ']}')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccbe63-9f63-4da0-8314-30244c7a8df1",
   "metadata": {},
   "source": [
    "You should now see .json data as required by matching engine\n",
    "![](jsonl.png)\n",
    "\n",
    "### Upload the data to GCS\n",
    "Only remove if you have issues uploading the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd0c7011-dbf2-4f73-adcc-89aff7ecb7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://movie_embeddings.json [Content-Type=application/json]...\n",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \n",
      "Operation completed over 1 objects/1.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp movie_embeddings.json $EMBEDDINGS/movie_embeddings.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1af7d-d6db-4bd6-91d6-f8343700ea94",
   "metadata": {},
   "source": [
    "# Next we will deploy our movie inidicies. With Matching Engine\n",
    "* Create an index (from the `json` files)\n",
    "* Create and endpoint\n",
    "* Deploy the index to the endpoint so you can perform vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c678c72-3604-4d32-b1f9-7639066932e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_endpoint_me = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "index_client = aiplatform_v1beta1.IndexServiceClient(\n",
    "    client_options=dict(api_endpoint=api_endpoint_me)\n",
    ")\n",
    "\n",
    "\n",
    "DISPLAY_NAME = f\"Movielens Movie: {DIMENSIONS} DIMENSIONS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49483180-e16a-48ac-9954-557dc2718144",
   "metadata": {},
   "source": [
    "Set the Nearest Neighbor Options\n",
    "\n",
    "See here for tips on [tuning the index](https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#tuning_the_index)\n",
    "\n",
    "Other best practices from our PM team:\n",
    "```\n",
    "Start from leafNodesToSearchPercent=5 and approximateNeighborsCount=10 * k\n",
    "\n",
    "use default values for others.\n",
    "\n",
    "measure performance and recall and change those 2 parameters accordingly.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76ed453f-d1de-4a53-af86-ebd74785d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "treeAhConfig = struct_pb2.Struct(\n",
    "    fields={\n",
    "        \"leafNodeEmbeddingCount\": struct_pb2.Value(number_value=20),\n",
    "        \"leafNodesToSearchPercent\": struct_pb2.Value(number_value=7),\n",
    "    }\n",
    ")\n",
    "\n",
    "algorithmConfig = struct_pb2.Struct(\n",
    "    fields={\"treeAhConfig\": struct_pb2.Value(struct_value=treeAhConfig)}\n",
    ")\n",
    "\n",
    "config = struct_pb2.Struct(\n",
    "    fields={\n",
    "        \"dimensions\": struct_pb2.Value(number_value=DIMENSIONS),\n",
    "        \"approximateNeighborsCount\": struct_pb2.Value(number_value=10),\n",
    "        \"distanceMeasureType\": struct_pb2.Value(string_value=\"DOT_PRODUCT_DISTANCE\"),\n",
    "        \"algorithmConfig\": struct_pb2.Value(struct_value=algorithmConfig),\n",
    "    }\n",
    ")\n",
    "\n",
    "metadata = struct_pb2.Struct(\n",
    "    fields={\n",
    "        \"config\": struct_pb2.Value(struct_value=config),\n",
    "        \"contentsDeltaUri\": struct_pb2.Value(string_value=EMBEDDINGS),\n",
    "    }\n",
    ")\n",
    "\n",
    "ann_index = {\n",
    "    \"display_name\": DISPLAY_NAME,\n",
    "    \"description\": f\"Movielens {DIMENSIONS}\",\n",
    "    \"metadata\": struct_pb2.Value(struct_value=metadata),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a429a9e9-85b4-420d-a6ac-200e45cb61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_index = index_client.create_index(parent=PARENT, index=ann_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c66dcd9-32e7-468b-97f2-8acf9ed3c1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n",
      "Poll the operation to create index...\n"
     ]
    }
   ],
   "source": [
    "# Poll the operation until it's done successfullly.\n",
    "# This will take ~40 min.\n",
    "import time \n",
    "\n",
    "while True:\n",
    "    if ann_index.done():\n",
    "        break\n",
    "    print(\"Poll the operation to create index...\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5016aed4-0ca8-4a08-86f4-aa86f5ca2a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.api_core.operation.Operation at 0x7f1557024050>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "355d7d95-6067-43fc-afc9-b9a75510d6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/679926387543/locations/us-central1/indexes/6165375113312075776\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_index.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af30e4-6572-4843-8539-f9801bf89609",
   "metadata": {},
   "source": [
    "### Save the name of the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34c7bcbe-6bc5-4740-ae7d-a7bc40389ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/679926387543/locations/us-central1/indexes/6165375113312075776'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_RESOURCE_NAME = ann_index.result().name\n",
    "INDEX_RESOURCE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bab2c7-c7ca-45d6-ae49-00903d13d0a7",
   "metadata": {},
   "source": [
    "Debugging tool in case you run into issues. Example usage below.\n",
    "`!gcloud beta ai operations describe 4122851463774863360 --index=7253099976438317056 --project=$PROJECT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae2c000-792b-4766-9562-fddf93387308",
   "metadata": {},
   "source": [
    "## Create Index Endpoint and Deploy Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "596eea7a-c339-45ee-9443-e4777e073bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/679926387543/global/networks/me-network'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VPC_NETWORK_NAME = \"projects/{}/global/networks/{}\".format(PROJECT_NUMBER, NETWORK_NAME)\n",
    "VPC_NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95f6267d-e920-475a-899e-25811f4eefbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint = {\n",
    "    \"display_name\": \"index_endpoint_for_demo\",\n",
    "    \"network\": VPC_NETWORK_NAME,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3959ed5b-8122-4840-8efc-6efed67f74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint_client = aiplatform_v1beta1.IndexEndpointServiceClient(\n",
    "    client_options=dict(api_endpoint=api_endpoint_me)\n",
    ")\n",
    "\n",
    "ann_index_en = index_endpoint_client.create_index_endpoint(\n",
    "    parent=PARENT, index_endpoint=index_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d70e1ef-2abd-43b1-827b-b884f34624df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/679926387543/locations/us-central1/indexEndpoints/5519108566784409600\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_index_en.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b36a5661-9fd5-4b9a-9922-6aae5b68af00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/679926387543/locations/us-central1/indexEndpoints/5519108566784409600'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_ENDPOINT_NAME = ann_index_en.result().name\n",
    "INDEX_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "beaf193d-bd0e-40be-80fa-dd4dbe641cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_INDEX_ID = 'movielens_deployed2'\n",
    "\n",
    "deploy_ann_index = {\n",
    "    \"id\": DEPLOYED_INDEX_ID,\n",
    "    \"display_name\": DEPLOYED_INDEX_ID,\n",
    "    \"index\": INDEX_RESOURCE_NAME,\n",
    "}\n",
    "r = index_endpoint_client.deploy_index(\n",
    "    index_endpoint=INDEX_ENDPOINT_NAME, deployed_index=deploy_ann_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "717b31a2-d4d9-40c5-9d49-224b120c4819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deployed_index {\n",
       "  id: \"movielens_deployed2\"\n",
       "}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc5328-aee5-4042-ba33-95604e74e196",
   "metadata": {},
   "source": [
    "# Connect Matching Engine and The User Model Into a Recommendation System\n",
    "\n",
    "This will bring it all together by incorporating the prediction endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3663265-9097-4ab5-8ed5-b6065ceecec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish index_endpoint -IMPORTANT for constructing already created endpoints/indicies/etc...\n",
    "ME_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(INDEX_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78b86481-e4fa-4c9c-b37d-8ab691f08574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.22754097,\n",
       "  -0.0064380914,\n",
       "  0.0135015259,\n",
       "  0.112238489,\n",
       "  -0.277818114,\n",
       "  -0.159358323,\n",
       "  -0.1638522,\n",
       "  -0.00327851763,\n",
       "  -0.159260809,\n",
       "  -0.491864324,\n",
       "  -0.193261221,\n",
       "  0.30505079,\n",
       "  -0.307912469,\n",
       "  -0.301744461,\n",
       "  0.23069109,\n",
       "  -0.361792147,\n",
       "  0.463443398,\n",
       "  0.0457165837,\n",
       "  -0.162908927,\n",
       "  -0.427746952,\n",
       "  0.636374176,\n",
       "  0.342389196,\n",
       "  -0.0582389496,\n",
       "  0.221517876,\n",
       "  -0.652715504,\n",
       "  0.216488883,\n",
       "  0.427579284,\n",
       "  0.488048613,\n",
       "  0.0215745568,\n",
       "  0.205890983,\n",
       "  0.0553605147,\n",
       "  -0.470034838,\n",
       "  0.0314469598,\n",
       "  -0.691070139,\n",
       "  0.0609962605,\n",
       "  -0.201734498,\n",
       "  -0.0754234344,\n",
       "  0.089362748,\n",
       "  -0.254325509,\n",
       "  -0.0444015339,\n",
       "  -0.0583328977,\n",
       "  0.243367925,\n",
       "  0.221339,\n",
       "  -0.136401564,\n",
       "  0.338161618,\n",
       "  -0.11122942,\n",
       "  0.248045325,\n",
       "  -0.294821292,\n",
       "  0.429468542,\n",
       "  0.0255604722,\n",
       "  1.00177443,\n",
       "  0.00296269078,\n",
       "  -0.268753201,\n",
       "  -0.240518704,\n",
       "  -0.247991562,\n",
       "  0.674329817,\n",
       "  -0.313638657,\n",
       "  -0.370729953,\n",
       "  -0.149122491,\n",
       "  0.338815,\n",
       "  -0.332382,\n",
       "  0.0227264874,\n",
       "  0.852791429,\n",
       "  0.235119537]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER = 627.0 #pick anyone 0-100k to see watch history and recommendations\n",
    "NUM_NEIGH=3\n",
    "\n",
    "emb_627 = endpoint.predict([[USER]]) #prediction from the saved model\n",
    "emb_627 = emb_627.predictions[0]\n",
    "emb_627 # we should get our user xxx embedding @ dim len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a974e272-28e6-4b9f-bf75-62bd41ce50bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[MatchNeighbor(id='1478.0', distance=4.327176094055176),\n",
       "  MatchNeighbor(id='1135.0', distance=4.176234722137451),\n",
       "  MatchNeighbor(id='1136.0', distance=4.097315311431885),\n",
       "  MatchNeighbor(id='1004.0', distance=3.9972083568573),\n",
       "  MatchNeighbor(id='939.0', distance=3.618990182876587),\n",
       "  MatchNeighbor(id='188.0', distance=3.5515005588531494),\n",
       "  MatchNeighbor(id='518.0', distance=3.525977611541748),\n",
       "  MatchNeighbor(id='461.0', distance=3.502216339111328),\n",
       "  MatchNeighbor(id='76.0', distance=3.4985222816467285),\n",
       "  MatchNeighbor(id='942.0', distance=3.492293357849121)]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ME_index_endpoint.match(queries=emb_627, deployed_index_id=DEPLOYED_INDEX_ID, num_neighbors=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f276c5-2899-4623-959c-9fadeda97cba",
   "metadata": {},
   "source": [
    "#### Create movie lookup tables\n",
    "Get what given user has rated highly, and what is being recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bade5a50-4525-405a-ae40-ec4fa06e6b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-25 19:30:56--  https://files.grouplens.org/datasets/movielens/ml-100k/u.item\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 236344 (231K)\n",
      "Saving to: ‘u.item’\n",
      "\n",
      "u.item              100%[===================>] 230.80K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2022-07-25 19:30:57 (2.92 MB/s) - ‘u.item’ saved [236344/236344]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://files.grouplens.org/datasets/movielens/ml-100k/u.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7621c74c-64d4-4d73-a083-53ee4c9a6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sidetour - create movie lookup dictionary\n",
    "movie_names = pd.read_csv('u.item', delimiter='|' , \n",
    "                          encoding='latin-1', \n",
    "                          usecols=(0,1),\n",
    "                          names = ['movie_id', 'title'])\n",
    "movielookup = movie_names.to_dict()['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c25fa63d-83d6-44d1-a60e-c67291771473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies watched: \n",
      " \n",
      "              0: Piano, The (1993)\n",
      "Movies watched: \n",
      " \n",
      "              1: Star Trek: The Wrath of Khan (1982)\n",
      "Movies watched: \n",
      " \n",
      "              2: Return of the Jedi (1983)\n",
      "Movies watched: \n",
      " \n",
      "              3: Star Trek VI: The Undiscovered Country (1991)\n",
      "Movies watched: \n",
      " \n",
      "              4: Star Trek III: The Search for Spock (1984)\n",
      "Movies watched: \n",
      " \n",
      "              5: Four Rooms (1995)\n",
      "Movies watched: \n",
      " \n",
      "              6: Addams Family Values (1993)\n",
      "Movies watched: \n",
      " \n",
      "              7: Arsenic and Old Lace (1944)\n",
      "Movies watched: \n",
      " \n",
      "              8: Pinocchio (1940)\n",
      "Movies watched: \n",
      " \n",
      "              9: Dead Poets Society (1989)\n"
     ]
    }
   ],
   "source": [
    "for i, watched_movie in enumerate(ratings.filter(lambda x: x['user_id']==USER)):\n",
    "    if i >= 10: #limit to top n\n",
    "        break\n",
    "    else:\n",
    "        key = watched_movie['movie_id'].numpy()\n",
    "        print(f\"\"\"Movies watched: \\n \n",
    "              {i}: {movielookup[key]}\"\"\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a7cb350-eb73-4e0a-aa9f-18afea2f32b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movie IDs: [[MatchNeighbor(id='1478.0', distance=4.327176094055176), MatchNeighbor(id='1135.0', distance=4.176234722137451), MatchNeighbor(id='1136.0', distance=4.097315311431885)]]\n"
     ]
    }
   ],
   "source": [
    "query_vector = emb_627\n",
    "\n",
    "\n",
    "ann_response = ME_index_endpoint.match(\n",
    "    deployed_index_id='movielens_deployed', \n",
    "    queries=query_vector, \n",
    "    num_neighbors=NUM_NEIGH\n",
    ")\n",
    "\n",
    "print(\"Recommended movie IDs:\", ann_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b02d9342-5e01-470a-810f-4a862d7a9608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies recommended: \n",
      " \n",
      "          0: Reckless (1995) (distance: 4.327176094055176)\n",
      "Movies recommended: \n",
      " \n",
      "          1: Ghosts of Mississippi (1996) (distance: 4.176234722137451)\n",
      "Movies recommended: \n",
      " \n",
      "          2: Beautiful Thing (1996) (distance: 4.097315311431885)\n"
     ]
    }
   ],
   "source": [
    "# look at the recommended movies vs the viewed for that user\n",
    "for i, match in enumerate(ann_response[0]):\n",
    "    key = int(float(match.id))\n",
    "    print(f\"\"\"Movies recommended: \\n \n",
    "          {i}: {movielookup[key]} (distance: {match.distance})\"\"\"\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e791683-6620-487d-bb2f-b783f6997886",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the tutorial. You can also manually delete resources that you created by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac66d7-b56c-4224-b847-fb29eb4fa045",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_RESOURCE_NAME\n",
    "# 7352179168240467968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f51e78-943e-43ae-8752-01843d5dd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61ecdd-df77-4939-ab93-2ed43dc8878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint_client.undeploy_index(index_endpoint=INDEX_ENDPOINT_NAME, deployed_index_id=DEPLOYED_INDEX_ID)\n",
    "\n",
    "index_client.delete_index(name=INDEX_RESOURCE_NAME)\n",
    "\n",
    "index_endpoint_client.delete_index_endpoint(name=INDEX_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037d03b-dfe6-43de-b2a7-ad01b3d4bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_resource_name = endpoint.resource_name\n",
    "endpoint_resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae4524-b080-486c-9a9f-16f5f9c21c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_resource_name = deployment.resource_name\n",
    "deployment_resource_name\n",
    "aiplatform.Endpoint.delete(endpoint, gcp_model)\n",
    "#delete our model endpoints, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41544df3-2341-439f-86a6-533826c6bf96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
